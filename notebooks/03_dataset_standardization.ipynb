{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72db072c-db8a-48e3-8272-628f60eca422",
   "metadata": {},
   "source": [
    "# BackTapBench: Dataset Standardization\n",
    "\n",
    "**Purpose**: Create standardized dataset from 5 participants\n",
    "\n",
    "**Input**: 5 participants √ó 9 grid positions = 45 CSV files\n",
    "\n",
    "**Output**: Standardized dataset in `backtapbench_standard/` folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4f34b2-b392-431b-b1b7-c1adef32262a",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71b66e-265b-4922-8494-4ad60285f76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import detection functions from previous notebook\n",
    "from tap_detection_functions import detect_taps_in_signal, extract_tap_segments\n",
    "\n",
    "# Set visual style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae964ce8-b515-4223-87f6-cdf2c19d60f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "DATA_PATH = \"data/\"\n",
    "OUTPUT_PATH = \"backtapbench_standard/\"\n",
    "PARTICIPANTS = [\"participant1\", \"participant2\", \"participant3\", \"participant4\", \"participant5\"]\n",
    "SENSOR_COLS = ['ax', 'ay', 'az', 'highX', 'highY', 'highZ', 'accelMag', 'gx', 'gy', 'gz']\n",
    "\n",
    "# Tap detection parameters (from notebook 02)\n",
    "WINDOW_SIZE = 6\n",
    "Z_ENERGY_THRESHOLD = 3.0\n",
    "GYRO_THRESHOLD = 1.0\n",
    "DEBOUNCE_MS = 120\n",
    "PRE_MS = 60\n",
    "POST_MS = 200\n",
    "SAMPLING_RATE = 120\n",
    "\n",
    "# Derived parameters\n",
    "PRE_SAMPLES = int(PRE_MS / 1000 * SAMPLING_RATE)\n",
    "POST_SAMPLES = int(POST_MS / 1000 * SAMPLING_RATE)\n",
    "DEBOUNCE_SAMPLES = int(DEBOUNCE_MS / 1000 * SAMPLING_RATE)\n",
    "SEGMENT_LENGTH = PRE_SAMPLES + POST_SAMPLES\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET STANDARDIZATION CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input data: {DATA_PATH}\")\n",
    "print(f\"Output folder: {OUTPUT_PATH}\")\n",
    "print(f\"Participants: {PARTICIPANTS}\")\n",
    "print(f\"Segment length: {SEGMENT_LENGTH} samples\")\n",
    "print(f\"Sensors: {len(SENSOR_COLS)} channels\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_PATH, \"features\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTPUT_PATH, \"splits\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d77219-5d95-40c3-912a-b5e62ac8678f",
   "metadata": {},
   "source": [
    "## 2. Standardization Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f22587-9bdd-431d-afd7-6c9d9b0cd041",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BackTapStandardizer:\n",
    "    \"\"\"\n",
    "    Standardizes BackTapBench dataset to consistent format\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=DATA_PATH, output_path=OUTPUT_PATH):\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.sensor_cols = SENSOR_COLS\n",
    "        self.segment_length = SEGMENT_LENGTH\n",
    "        \n",
    "        # Data containers\n",
    "        self.all_segments = []      # List of segment arrays\n",
    "        self.all_labels = []        # Grid positions (0-8)\n",
    "        self.all_participants = []  # Participant IDs\n",
    "        self.all_grids = []         # Grid positions (redundant with labels but useful)\n",
    "        self.all_trial_nums = []    # Original trial numbers\n",
    "        self.metadata = []          # Detailed metadata for each segment\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            'total_segments': 0,\n",
    "            'segments_per_participant': defaultdict(int),\n",
    "            'segments_per_grid': defaultdict(int),\n",
    "            'segment_shapes': []\n",
    "        }\n",
    "    \n",
    "    def process_all_participants(self):\n",
    "        \"\"\"\n",
    "        Process all participants and create standardized dataset\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"PROCESSING ALL PARTICIPANTS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        total_files_processed = 0\n",
    "        total_segments_extracted = 0\n",
    "        \n",
    "        for participant in PARTICIPANTS:\n",
    "            participant_path = os.path.join(self.data_path, participant)\n",
    "            \n",
    "            if not os.path.exists(participant_path):\n",
    "                print(f\"‚ùå Skipping {participant}: folder not found\")\n",
    "                continue\n",
    "            \n",
    "            # Get all CSV files for this participant\n",
    "            csv_files = sorted([f for f in os.listdir(participant_path) \n",
    "                              if f.endswith('.csv')])\n",
    "            \n",
    "            print(f\"\\nüìÅ Processing {participant}: {len(csv_files)} files\")\n",
    "            \n",
    "            for file_idx, filename in enumerate(csv_files):\n",
    "                # Extract grid position from filename\n",
    "                # Format: participantX_trialYYYY.csv\n",
    "                try:\n",
    "                    trial_num = int(filename.split('trial')[1].split('.')[0])\n",
    "                    grid_position = trial_num - 1  # trial0001 = grid0\n",
    "                except:\n",
    "                    print(f\"   ‚ö†Ô∏è  Could not parse grid from {filename}, using file index\")\n",
    "                    grid_position = file_idx\n",
    "                \n",
    "                # Process this file\n",
    "                segments = self._process_single_file(participant, filename, grid_position)\n",
    "                \n",
    "                total_files_processed += 1\n",
    "                total_segments_extracted += len(segments)\n",
    "                \n",
    "                print(f\"   Grid {grid_position}: {len(segments)} segments extracted\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total files processed: {total_files_processed}\")\n",
    "        print(f\"Total segments extracted: {total_segments_extracted}\")\n",
    "        \n",
    "        # Convert lists to arrays\n",
    "        self._finalize_data()\n",
    "        \n",
    "        return total_segments_extracted\n",
    "    \n",
    "    def _process_single_file(self, participant, filename, grid_position):\n",
    "        \"\"\"\n",
    "        Process a single CSV file and extract tap segments\n",
    "        \"\"\"\n",
    "        filepath = os.path.join(self.data_path, participant, filename)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(filepath)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error reading {filename}: {e}\")\n",
    "            return []\n",
    "        \n",
    "        # Calculate gyro magnitude\n",
    "        gyro_mag = np.sqrt(df['gx']**2 + df['gy']**2 + df['gz']**2)\n",
    "        \n",
    "        # Detect taps\n",
    "        tap_indices = detect_taps_in_signal(\n",
    "            df['highZ'].values, \n",
    "            gyro_mag,\n",
    "            window_size=WINDOW_SIZE,\n",
    "            z_threshold=Z_ENERGY_THRESHOLD,\n",
    "            gyro_threshold=GYRO_THRESHOLD,\n",
    "            debounce_samples=DEBOUNCE_SAMPLES\n",
    "        )\n",
    "        \n",
    "        # Extract segments\n",
    "        segments = extract_tap_segments(\n",
    "            df, \n",
    "            tap_indices,\n",
    "            pre_samples=PRE_SAMPLES,\n",
    "            post_samples=POST_SAMPLES,\n",
    "            sensor_cols=self.sensor_cols\n",
    "        )\n",
    "        \n",
    "        # Store segments and metadata\n",
    "        for seg_idx, segment in enumerate(segments):\n",
    "            # Validate segment shape\n",
    "            if segment.shape[0] != self.segment_length:\n",
    "                print(f\"   ‚ö†Ô∏è  Segment {seg_idx} has wrong length: {segment.shape[0]} != {self.segment_length}\")\n",
    "                # Pad or truncate to correct length\n",
    "                if segment.shape[0] < self.segment_length:\n",
    "                    pad_size = self.segment_length - segment.shape[0]\n",
    "                    segment = np.pad(segment, ((0, pad_size), (0, 0)), mode='edge')\n",
    "                else:\n",
    "                    segment = segment[:self.segment_length, :]\n",
    "            \n",
    "            self.all_segments.append(segment)\n",
    "            self.all_labels.append(grid_position)\n",
    "            self.all_participants.append(participant)\n",
    "            self.all_grids.append(grid_position)\n",
    "            self.all_trial_nums.append(int(filename.split('trial')[1].split('.')[0]))\n",
    "            \n",
    "            # Store detailed metadata\n",
    "            self.metadata.append({\n",
    "                'participant': participant,\n",
    "                'original_file': filename,\n",
    "                'grid_position': grid_position,\n",
    "                'segment_id': seg_idx,\n",
    "                'segment_length': segment.shape[0],\n",
    "                'sampling_rate': SAMPLING_RATE,\n",
    "                'tap_index_original': tap_indices[seg_idx] if seg_idx < len(tap_indices) else -1,\n",
    "                'sensor_columns': self.sensor_cols\n",
    "            })\n",
    "            \n",
    "            # Update statistics\n",
    "            self.stats['segments_per_participant'][participant] += 1\n",
    "            self.stats['segments_per_grid'][grid_position] += 1\n",
    "            self.stats['segment_shapes'].append(segment.shape)\n",
    "        \n",
    "        return segments\n",
    "    \n",
    "    def _finalize_data(self):\n",
    "        \"\"\"\n",
    "        Convert lists to numpy arrays and update statistics\n",
    "        \"\"\"\n",
    "        # Convert to numpy arrays\n",
    "        self.segments_array = np.array(self.all_segments)  # Shape: (n_segments, segment_length, n_sensors)\n",
    "        self.labels_array = np.array(self.all_labels)\n",
    "        self.participants_array = np.array(self.all_participants)\n",
    "        self.grids_array = np.array(self.all_grids)\n",
    "        \n",
    "        # Update statistics\n",
    "        self.stats['total_segments'] = len(self.all_segments)\n",
    "        self.stats['unique_participants'] = len(set(self.all_participants))\n",
    "        self.stats['unique_grids'] = len(set(self.all_grids))\n",
    "        self.stats['segment_shape_final'] = self.segments_array.shape\n",
    "        \n",
    "        # Calculate segment length consistency\n",
    "        segment_lengths = [seg.shape[0] for seg in self.all_segments]\n",
    "        self.stats['segment_length_mean'] = np.mean(segment_lengths)\n",
    "        self.stats['segment_length_std'] = np.std(segment_lengths)\n",
    "    \n",
    "    def save_standardized_data(self):\n",
    "        \"\"\"\n",
    "        Save standardized data to output folder\n",
    "        \"\"\"\n",
    "        print(\"\\nüíæ SAVING STANDARDIZED DATA...\")\n",
    "        \n",
    "        # Save main dataset as compressed numpy file\n",
    "        np.savez_compressed(\n",
    "            os.path.join(self.output_path, 'backtapbench_data.npz'),\n",
    "            segments=self.segments_array,\n",
    "            labels=self.labels_array,\n",
    "            participants=self.participants_array,\n",
    "            grids=self.grids_array,\n",
    "            trial_numbers=np.array(self.all_trial_nums)\n",
    "        )\n",
    "        print(f\"‚úÖ Main data saved: backtapbench_data.npz\")\n",
    "        print(f\"   Shape: {self.segments_array.shape}\")\n",
    "        print(f\"   Segments: {len(self.segments_array)}\")\n",
    "        print(f\"   Labels: {len(self.labels_array)} (0-{np.max(self.labels_array)})\")\n",
    "        \n",
    "        # Save metadata as JSON\n",
    "        with open(os.path.join(self.output_path, 'metadata.json'), 'w') as f:\n",
    "            json.dump(self.metadata, f, indent=2)\n",
    "        print(f\"‚úÖ Metadata saved: metadata.json ({len(self.metadata)} entries)\")\n",
    "        \n",
    "        # Save statistics\n",
    "        with open(os.path.join(self.output_path, 'dataset_statistics.json'), 'w') as f:\n",
    "            json.dump(self.stats, f, indent=2, default=str)\n",
    "        print(f\"‚úÖ Statistics saved: dataset_statistics.json\")\n",
    "        \n",
    "        # Save as pickle for Python convenience\n",
    "        dataset_dict = {\n",
    "            'segments': self.segments_array,\n",
    "            'labels': self.labels_array,\n",
    "            'participants': self.participants_array,\n",
    "            'grids': self.grids_array,\n",
    "            'sensor_columns': self.sensor_cols,\n",
    "            'segment_length': self.segment_length,\n",
    "            'sampling_rate': SAMPLING_RATE,\n",
    "            'metadata': self.metadata\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(self.output_path, 'backtapbench_dataset.pkl'), 'wb') as f:\n",
    "            pickle.dump(dataset_dict, f)\n",
    "        print(f\"‚úÖ Pickle file saved: backtapbench_dataset.pkl\")\n",
    "        \n",
    "        # Create a simplified CSV version for quick inspection\n",
    "        self._create_summary_csv()\n",
    "        \n",
    "        # Extract features for classical ML\n",
    "        self._extract_and_save_features()\n",
    "        \n",
    "        # Create dataset splits\n",
    "        self._create_dataset_splits()\n",
    "        \n",
    "        # Generate README\n",
    "        self._generate_readme()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STANDARDIZATION COMPLETE!\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def _create_summary_csv(self):\n",
    "        \"\"\"\n",
    "        Create a CSV summary of the dataset\n",
    "        \"\"\"\n",
    "        summary_data = []\n",
    "        for i in range(len(self.all_segments)):\n",
    "            summary_data.append({\n",
    "                'segment_id': i,\n",
    "                'participant': self.all_participants[i],\n",
    "                'grid_position': self.all_grids[i],\n",
    "                'label': self.all_labels[i],\n",
    "                'original_file': self.metadata[i]['original_file'],\n",
    "                'segment_length': self.metadata[i]['segment_length']\n",
    "            })\n",
    "        \n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(os.path.join(self.output_path, 'dataset_summary.csv'), index=False)\n",
    "        print(f\"‚úÖ Summary CSV saved: dataset_summary.csv\")\n",
    "    \n",
    "    def _extract_and_save_features(self):\n",
    "        \"\"\"\n",
    "        Extract hand-crafted features for classical ML algorithms\n",
    "        \"\"\"\n",
    "        print(f\"\\nüîß EXTRACTING FEATURES...\")\n",
    "        \n",
    "        feature_rows = []\n",
    "        feature_names = []\n",
    "        \n",
    "        for i, segment in enumerate(self.segments_array):\n",
    "            features = self._extract_segment_features(segment)\n",
    "            features['label'] = self.labels_array[i]\n",
    "            features['participant'] = self.all_participants[i]\n",
    "            features['grid_position'] = self.grids_array[i]\n",
    "            features['segment_id'] = i\n",
    "            \n",
    "            feature_rows.append(features)\n",
    "            \n",
    "            # Store feature names (once)\n",
    "            if not feature_names:\n",
    "                feature_names = list(features.keys())\n",
    "        \n",
    "        # Create feature DataFrame\n",
    "        feature_df = pd.DataFrame(feature_rows)\n",
    "        \n",
    "        # Save features\n",
    "        feature_df.to_csv(os.path.join(self.output_path, 'features', 'statistical_features.csv'), index=False)\n",
    "        \n",
    "        # Save feature names\n",
    "        with open(os.path.join(self.output_path, 'features', 'feature_names.json'), 'w') as f:\n",
    "            json.dump(feature_names, f, indent=2)\n",
    "        \n",
    "        # Save feature matrix (without metadata columns)\n",
    "        feature_cols = [col for col in feature_df.columns \n",
    "                       if col not in ['label', 'participant', 'grid_position', 'segment_id']]\n",
    "        feature_matrix = feature_df[feature_cols].values\n",
    "        np.save(os.path.join(self.output_path, 'features', 'feature_matrix.npy'), feature_matrix)\n",
    "        \n",
    "        print(f\"‚úÖ Features saved:\")\n",
    "        print(f\"   - statistical_features.csv ({len(feature_df)} samples, {len(feature_cols)} features)\")\n",
    "        print(f\"   - feature_matrix.npy ({feature_matrix.shape})\")\n",
    "        print(f\"   - feature_names.json\")\n",
    "    \n",
    "    def _extract_segment_features(self, segment):\n",
    "        \"\"\"\n",
    "        Extract statistical features from a segment\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # For each sensor channel\n",
    "        for i, col in enumerate(self.sensor_cols):\n",
    "            channel_data = segment[:, i]\n",
    "            \n",
    "            # Basic statistical features\n",
    "            features[f'{col}_mean'] = np.mean(channel_data)\n",
    "            features[f'{col}_std'] = np.std(channel_data)\n",
    "            features[f'{col}_min'] = np.min(channel_data)\n",
    "            features[f'{col}_max'] = np.max(channel_data)\n",
    "            features[f'{col}_range'] = np.ptp(channel_data)  # peak-to-peak\n",
    "            features[f'{col}_median'] = np.median(channel_data)\n",
    "            features[f'{col}_energy'] = np.sum(channel_data ** 2)\n",
    "            features[f'{col}_abs_energy'] = np.sum(np.abs(channel_data))\n",
    "            \n",
    "            # Shape features\n",
    "            from scipy import stats\n",
    "            features[f'{col}_skew'] = stats.skew(channel_data)\n",
    "            features[f'{col}_kurtosis'] = stats.kurtosis(channel_data)\n",
    "            \n",
    "            # Temporal features\n",
    "            features[f'{col}_zero_crossings'] = np.sum(np.diff(np.sign(channel_data)) != 0)\n",
    "            features[f'{col}_mean_crossings'] = np.sum(np.diff(np.sign(channel_data - np.mean(channel_data))) != 0)\n",
    "            \n",
    "            # Peak features\n",
    "            from scipy.signal import find_peaks\n",
    "            peaks, _ = find_peaks(np.abs(channel_data), height=np.std(channel_data))\n",
    "            features[f'{col}_num_peaks'] = len(peaks)\n",
    "            if len(peaks) > 0:\n",
    "                features[f'{col}_peak_mean'] = np.mean(np.abs(channel_data[peaks]))\n",
    "                features[f'{col}_peak_std'] = np.std(np.abs(channel_data[peaks]))\n",
    "            else:\n",
    "                features[f'{col}_peak_mean'] = 0\n",
    "                features[f'{col}_peak_std'] = 0\n",
    "        \n",
    "        # Cross-channel features\n",
    "        # Accelerometer magnitude features (already have accelMag channel)\n",
    "        accel_mag = segment[:, 6]  # accelMag is column 6\n",
    "        features['accelMag_mean'] = np.mean(accel_mag)\n",
    "        features['accelMag_std'] = np.std(accel_mag)\n",
    "        features['accelMag_energy'] = np.sum(accel_mag ** 2)\n",
    "        \n",
    "        # Gyroscope magnitude\n",
    "        gx, gy, gz = segment[:, 7], segment[:, 8], segment[:, 9]\n",
    "        gyro_mag = np.sqrt(gx**2 + gy**2 + gz**2)\n",
    "        features['gyroMag_mean'] = np.mean(gyro_mag)\n",
    "        features['gyroMag_std'] = np.std(gyro_mag)\n",
    "        features['gyroMag_energy'] = np.sum(gyro_mag ** 2)\n",
    "        \n",
    "        # Ratio features\n",
    "        features['highZ_to_accelMag_ratio'] = features['highZ_energy'] / (features['accelMag_energy'] + 1e-10)\n",
    "        features['highZ_to_gyroMag_ratio'] = features['highZ_energy'] / (features['gyroMag_energy'] + 1e-10)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _create_dataset_splits(self):\n",
    "        \"\"\"\n",
    "        Create standard dataset splits for evaluation\n",
    "        \"\"\"\n",
    "        print(f\"\\nüìä CREATING DATASET SPLITS...\")\n",
    "        \n",
    "        splits = {}\n",
    "        \n",
    "        # 1. Within-participant splits (5-fold CV per participant)\n",
    "        for participant in set(self.all_participants):\n",
    "            participant_indices = np.where(self.participants_array == participant)[0]\n",
    "            n_samples = len(participant_indices)\n",
    "            \n",
    "            # Create 5 folds\n",
    "            np.random.seed(42)\n",
    "            np.random.shuffle(participant_indices)\n",
    "            folds = np.array_split(participant_indices, 5)\n",
    "            \n",
    "            splits[f'within_{participant}'] = {\n",
    "                'folds': [f.tolist() for f in folds],\n",
    "                'n_samples': n_samples,\n",
    "                'type': 'within_participant'\n",
    "            }\n",
    "        \n",
    "        # 2. Cross-participant splits (Leave-One-Participant-Out)\n",
    "        all_participants = list(set(self.all_participants))\n",
    "        for i, test_participant in enumerate(all_participants):\n",
    "            train_indices = []\n",
    "            test_indices = []\n",
    "            \n",
    "            for j, participant in enumerate(all_participants):\n",
    "                indices = np.where(self.participants_array == participant)[0]\n",
    "                if participant == test_participant:\n",
    "                    test_indices.extend(indices.tolist())\n",
    "                else:\n",
    "                    train_indices.extend(indices.tolist())\n",
    "            \n",
    "            splits[f'lopout_{test_participant}'] = {\n",
    "                'train': train_indices,\n",
    "                'test': test_indices,\n",
    "                'n_train': len(train_indices),\n",
    "                'n_test': len(test_indices),\n",
    "                'type': 'leave_one_participant_out'\n",
    "            }\n",
    "        \n",
    "        # 3. Random 80/20 split (for quick experiments)\n",
    "        all_indices = np.arange(len(self.segments_array))\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(all_indices)\n",
    "        split_idx = int(0.8 * len(all_indices))\n",
    "        splits['random_80_20'] = {\n",
    "            'train': all_indices[:split_idx].tolist(),\n",
    "            'test': all_indices[split_idx:].tolist(),\n",
    "            'n_train': split_idx,\n",
    "            'n_test': len(all_indices) - split_idx,\n",
    "            'type': 'random_split'\n",
    "        }\n",
    "        \n",
    "        # Save splits\n",
    "        with open(os.path.join(self.output_path, 'splits', 'dataset_splits.json'), 'w') as f:\n",
    "            json.dump(splits, f, indent=2)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset splits saved: dataset_splits.json\")\n",
    "        print(f\"   - Within-participant folds: {len([k for k in splits.keys() if k.startswith('within_')])}\")\n",
    "        print(f\"   - Leave-one-participant-out: {len([k for k in splits.keys() if k.startswith('lopout_')])}\")\n",
    "        print(f\"   - Random 80/20 split: 1\")\n",
    "\n",
    "    def _generate_readme(self):\n",
    "        \"\"\"\n",
    "        Generate README file for the standardized dataset\n",
    "        \"\"\"\n",
    "        # Calculate statistics\n",
    "        segments_per_participant = {p: np.sum(self.participants_array == p) \n",
    "                                   for p in set(self.participants_array)}\n",
    "        segments_per_grid = {g: np.sum(self.grids_array == g) for g in range(9)}\n",
    "        \n",
    "        # Build content piece by piece to avoid f-string issues\n",
    "        content_parts = []\n",
    "        \n",
    "        # Header\n",
    "        content_parts.append(\"# BackTapBench Standardized Dataset\\n\")\n",
    "        \n",
    "        # Overview\n",
    "        content_parts.append(\"## Overview\\n\")\n",
    "        content_parts.append(\"This dataset contains standardized back-of-device tap data for benchmarking tap recognition algorithms.\\n\")\n",
    "        \n",
    "        # Dataset Statistics\n",
    "        content_parts.append(\"## Dataset Statistics\\n\")\n",
    "        content_parts.append(f\"- **Total segments**: {self.stats['total_segments']}\\n\")\n",
    "        content_parts.append(f\"- **Participants**: {self.stats['unique_participants']} ({', '.join(sorted(set(self.all_participants)))})\\n\")\n",
    "        content_parts.append(f\"- **Grid positions**: {self.stats['unique_grids']} (0-8)\\n\")\n",
    "        content_parts.append(f\"- **Segment shape**: {self.segments_array.shape[1:]} (samples √ó sensors)\\n\")\n",
    "        content_parts.append(f\"- **Sampling rate**: {SAMPLING_RATE} Hz\\n\")\n",
    "        content_parts.append(f\"- **Segment duration**: {self.segment_length * 1000 / SAMPLING_RATE:.0f}ms ({PRE_MS}ms pre-tap + {POST_MS}ms post-tap)\\n\")\n",
    "        \n",
    "        # File Structure\n",
    "        content_parts.append(\"## File Structure\\n\")\n",
    "        content_parts.append(\"```\\n\")\n",
    "        content_parts.append(\"backtapbench_standard/\\n\")\n",
    "        content_parts.append(\"‚îú‚îÄ‚îÄ backtapbench_data.npz          # Main dataset (segments, labels, participants, grids)\\n\")\n",
    "        content_parts.append(\"‚îú‚îÄ‚îÄ backtapbench_dataset.pkl       # Python pickle with all data\\n\")\n",
    "        content_parts.append(\"‚îú‚îÄ‚îÄ metadata.json                  # Detailed metadata for each segment\\n\")\n",
    "        content_parts.append(\"‚îú‚îÄ‚îÄ dataset_statistics.json        # Dataset statistics\\n\")\n",
    "        content_parts.append(\"‚îú‚îÄ‚îÄ dataset_summary.csv            # CSV summary\\n\")\n",
    "        content_parts.append(\"‚îú‚îÄ‚îÄ README.md                      # This file\\n\")\n",
    "        content_parts.append(\"‚îú‚îÄ‚îÄ features/                      # Extracted features for classical ML\\n\")\n",
    "        content_parts.append(\"‚îÇ   ‚îú‚îÄ‚îÄ statistical_features.csv\\n\")\n",
    "        content_parts.append(\"‚îÇ   ‚îú‚îÄ‚îÄ feature_matrix.npy\\n\")\n",
    "        content_parts.append(\"‚îÇ   ‚îî‚îÄ‚îÄ feature_names.json\\n\")\n",
    "        content_parts.append(\"‚îî‚îÄ‚îÄ splits/                        # Pre-defined dataset splits\\n\")\n",
    "        content_parts.append(\"    ‚îî‚îÄ‚îÄ dataset_splits.json\\n\")\n",
    "        content_parts.append(\"```\\n\")\n",
    "        \n",
    "        # Data Format\n",
    "        content_parts.append(\"## Data Format\\n\")\n",
    "        content_parts.append(\"### Main Dataset (backtapbench_data.npz)\\n\")\n",
    "        content_parts.append(\"```python\\n\")\n",
    "        content_parts.append(\"import numpy as np\\n\")\n",
    "        content_parts.append(\"data = np.load('backtapbench_standard/backtapbench_data.npz')\\n\")\n",
    "        content_parts.append(\"segments = data['segments']      # Shape: (n_samples, 31, 10)\\n\")\n",
    "        content_parts.append(\"labels = data['labels']          # Grid positions (0-8)\\n\")\n",
    "        content_parts.append(\"participants = data['participants']  # Participant IDs\\n\")\n",
    "        content_parts.append(\"grids = data['grids']            # Grid positions (same as labels)\\n\")\n",
    "        content_parts.append(\"```\\n\")\n",
    "        \n",
    "        # Sensor Channels\n",
    "        content_parts.append(\"### Sensor Channels\\n\")\n",
    "        content_parts.append(\"| Index | Sensor | Description |\\n\")\n",
    "        content_parts.append(\"|-------|--------|-------------|\\n\")\n",
    "        sensor_descriptions = [\n",
    "            (\"ax\", \"Accelerometer X\"),\n",
    "            (\"ay\", \"Accelerometer Y\"),\n",
    "            (\"az\", \"Accelerometer Z\"),\n",
    "            (\"highX\", \"High-frequency accelerometer X\"),\n",
    "            (\"highY\", \"High-frequency accelerometer Y\"),\n",
    "            (\"highZ\", \"High-frequency accelerometer Z (used for tap detection)\"),\n",
    "            (\"accelMag\", \"Accelerometer magnitude\"),\n",
    "            (\"gx\", \"Gyroscope X\"),\n",
    "            (\"gy\", \"Gyroscope Y\"),\n",
    "            (\"gz\", \"Gyroscope Z\")\n",
    "        ]\n",
    "        \n",
    "        for i, (sensor, desc) in enumerate(sensor_descriptions):\n",
    "            content_parts.append(f\"| {i} | {sensor} | {desc} |\\n\")\n",
    "        \n",
    "        # Citation\n",
    "        content_parts.append(\"## Citation\\n\")\n",
    "        content_parts.append(\"If you use this dataset, please cite:\\n\")\n",
    "        content_parts.append(\"```\\n\")\n",
    "        content_parts.append(\"BackTapBench: An Open Benchmark for Back-of-Device Tap Recognition\\n\")\n",
    "        content_parts.append(\"[Your Name], [Your Institution], [Year]\\n\")\n",
    "        content_parts.append(\"```\\n\")\n",
    "        \n",
    "        # Join all parts\n",
    "        readme_content = ''.join(content_parts)\n",
    "        \n",
    "        with open(os.path.join(self.output_path, 'README.md'), 'w') as f:\n",
    "            f.write(readme_content)\n",
    "        \n",
    "        print(f\"‚úÖ README generated: README.md\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32afd36b-d6dd-4749-839d-0ed931f80268",
   "metadata": {},
   "source": [
    "## 3. Run Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb84f2e0-fb69-41eb-a2a1-ca9484d2cc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize standardizer\n",
    "standardizer = BackTapStandardizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61aa9d0e-1c26-41e9-a892-70a998e7195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all participants\n",
    "print(\"Starting dataset standardization...\")\n",
    "total_segments = standardizer.process_all_participants()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f800bfd9-0cec-43a1-ac34-d40753db1f4f",
   "metadata": {},
   "source": [
    "## 4. Visualize Data Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82af455-2285-4db8-b240-f9debf7d0d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data_distribution(standardizer):\n",
    "    \"\"\"\n",
    "    Visualize the distribution of the standardized dataset\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('BackTapBench Standardized Dataset Distribution', fontsize=16, y=1.02)\n",
    "    \n",
    "    # 1. Segments per participant\n",
    "    participant_counts = standardizer.stats['segments_per_participant']\n",
    "    axes[0, 0].bar(range(len(participant_counts)), list(participant_counts.values()))\n",
    "    axes[0, 0].set_xticks(range(len(participant_counts)))\n",
    "    axes[0, 0].set_xticklabels(list(participant_counts.keys()), rotation=45)\n",
    "    axes[0, 0].set_xlabel('Participant')\n",
    "    axes[0, 0].set_ylabel('Number of Segments')\n",
    "    axes[0, 0].set_title('Segments per Participant')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(participant_counts.values()):\n",
    "        axes[0, 0].text(i, v + 5, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Segments per grid position\n",
    "    grid_counts = standardizer.stats['segments_per_grid']\n",
    "    grid_positions = sorted(grid_counts.keys())\n",
    "    grid_values = [grid_counts[g] for g in grid_positions]\n",
    "    \n",
    "    axes[0, 1].bar(grid_positions, grid_values)\n",
    "    axes[0, 1].set_xlabel('Grid Position')\n",
    "    axes[0, 1].set_ylabel('Number of Segments')\n",
    "    axes[0, 1].set_title('Segments per Grid Position')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add grid labels\n",
    "    grid_labels = ['TL', 'TM', 'TR', 'ML', 'MM', 'MR', 'BL', 'BM', 'BR']\n",
    "    axes[0, 1].set_xticks(grid_positions)\n",
    "    axes[0, 1].set_xticklabels(grid_labels)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, v in enumerate(grid_values):\n",
    "        axes[0, 1].text(i, v + 5, str(v), ha='center', va='bottom')\n",
    "    \n",
    "    # 3. Segment length distribution\n",
    "    segment_lengths = [seg.shape[0] for seg in standardizer.all_segments]\n",
    "    axes[0, 2].hist(segment_lengths, bins=20, edgecolor='black', alpha=0.7)\n",
    "    axes[0, 2].axvline(x=SEGMENT_LENGTH, color='r', linestyle='--', \n",
    "                      label=f'Target: {SEGMENT_LENGTH}')\n",
    "    axes[0, 2].set_xlabel('Segment Length (samples)')\n",
    "    axes[0, 2].set_ylabel('Count')\n",
    "    axes[0, 2].set_title('Segment Length Distribution')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Participant √ó Grid heatmap\n",
    "    heatmap_data = np.zeros((len(PARTICIPANTS), 9))\n",
    "    \n",
    "    for i, participant in enumerate(PARTICIPANTS):\n",
    "        for j in range(9):\n",
    "            # Count segments for this participant and grid\n",
    "            count = np.sum((standardizer.participants_array == participant) & \n",
    "                          (standardizer.grids_array == j))\n",
    "            heatmap_data[i, j] = count\n",
    "    \n",
    "    im = axes[1, 0].imshow(heatmap_data, cmap='YlOrRd', aspect='auto')\n",
    "    axes[1, 0].set_xlabel('Grid Position')\n",
    "    axes[1, 0].set_ylabel('Participant')\n",
    "    axes[1, 0].set_title('Segments per Participant √ó Grid')\n",
    "    axes[1, 0].set_xticks(range(9))\n",
    "    axes[1, 0].set_xticklabels(grid_labels)\n",
    "    axes[1, 0].set_yticks(range(len(PARTICIPANTS)))\n",
    "    axes[1, 0].set_yticklabels(PARTICIPANTS)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(PARTICIPANTS)):\n",
    "        for j in range(9):\n",
    "            text = axes[1, 0].text(j, i, int(heatmap_data[i, j]),\n",
    "                                  ha=\"center\", va=\"center\", color=\"black\")\n",
    "    \n",
    "    plt.colorbar(im, ax=axes[1, 0])\n",
    "    \n",
    "    # 5. Sample segments visualization\n",
    "    if len(standardizer.all_segments) > 0:\n",
    "        # Show first few segments' highZ channel\n",
    "        n_samples = min(10, len(standardizer.all_segments))\n",
    "        for i in range(n_samples):\n",
    "            axes[1, 1].plot(standardizer.all_segments[i][:, 5], alpha=0.5, linewidth=0.5)\n",
    "        \n",
    "        axes[1, 1].axvline(x=PRE_SAMPLES, color='r', linestyle='--', \n",
    "                          label='Tap Center')\n",
    "        axes[1, 1].set_xlabel('Samples')\n",
    "        axes[1, 1].set_ylabel('HighZ')\n",
    "        axes[1, 1].set_title(f'Sample Segments (HighZ, n={n_samples})')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Label distribution pie chart\n",
    "    label_counts = np.bincount(standardizer.labels_array)\n",
    "    axes[1, 2].pie(label_counts, labels=grid_labels, autopct='%1.1f%%',\n",
    "                  startangle=90, colors=sns.color_palette(\"husl\", 9))\n",
    "    axes[1, 2].set_title('Distribution of Grid Positions')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTPUT_PATH, 'dataset_distribution.png'), \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Distribution plot saved: dataset_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d06b7-8830-4315-80a8-4b3428011535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize if we have data\n",
    "if total_segments > 0:\n",
    "    visualize_data_distribution(standardizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fedf46-f934-4928-a8ac-dd9f3c4e1781",
   "metadata": {},
   "source": [
    "## 5. Save Standardized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c31527-09a9-4da8-a75a-1f5c752d7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the standardized dataset\n",
    "standardizer.save_standardized_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521b503-0a55-4e9f-b615-7882c6fd1b14",
   "metadata": {},
   "source": [
    "## 6. Data Loading Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cbb3a-df87-4542-9aa4-408564f6c997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_data_loading():\n",
    "    \"\"\"\n",
    "    Demonstrate how to load and use the standardized dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA LOADING EXAMPLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Method 1: Load from NPZ\n",
    "    print(\"\\nüì• METHOD 1: Load from NPZ file\")\n",
    "    print(\"```python\")\n",
    "    print(\"import numpy as np\")\n",
    "    print(\"data = np.load('backtapbench_standard/backtapbench_data.npz')\")\n",
    "    print(\"segments = data['segments']\")\n",
    "    print(\"labels = data['labels']\")\n",
    "    print(\"participants = data['participants']\")\n",
    "    print(\"print(f'Segments shape: {segments.shape}')\")\n",
    "    print(\"print(f'Labels: {len(labels)}')\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # Method 2: Load from pickle\n",
    "    print(\"\\nüì• METHOD 2: Load from pickle\")\n",
    "    print(\"```python\")\n",
    "    print(\"import pickle\")\n",
    "    print(\"with open('backtapbench_standard/backtapbench_dataset.pkl', 'rb') as f:\")\n",
    "    print(\"    dataset = pickle.load(f)\")\n",
    "    print(\"segments = dataset['segments']\")\n",
    "    print(\"labels = dataset['labels']\")\n",
    "    print(\"sensor_columns = dataset['sensor_columns']\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # Method 3: Load features\n",
    "    print(\"\\nüì• METHOD 3: Load extracted features\")\n",
    "    print(\"```python\")\n",
    "    print(\"import pandas as pd\")\n",
    "    print(\"features = pd.read_csv('backtapbench_standard/features/statistical_features.csv')\")\n",
    "    print(\"X = features.drop(['label', 'participant', 'grid_position', 'segment_id'], axis=1)\")\n",
    "    print(\"y = features['label']\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # Method 4: Load splits\n",
    "    print(\"\\nüì• METHOD 4: Load pre-defined splits\")\n",
    "    print(\"```python\")\n",
    "    print(\"import json\")\n",
    "    print(\"with open('backtapbench_standard/splits/dataset_splits.json', 'r') as f:\")\n",
    "    print(\"    splits = json.load(f)\")\n",
    "    print(\"# Example: Get LOPO split for participant1\")\n",
    "    print(\"split = splits['lopout_participant1']\")\n",
    "    print(\"train_indices = split['train']\")\n",
    "    print(\"test_indices = split['test']\")\n",
    "    print(\"```\")\n",
    "    \n",
    "    # Actual loading example\n",
    "    print(\"\\nüîß ACTUAL LOADING DEMONSTRATION:\")\n",
    "    \n",
    "    try:\n",
    "        # Load the data we just created\n",
    "        data_path = os.path.join(OUTPUT_PATH, 'backtapbench_data.npz')\n",
    "        data = np.load(data_path)\n",
    "        \n",
    "        print(f\"Loaded data keys: {list(data.keys())}\")\n",
    "        print(f\"Segments shape: {data['segments'].shape}\")\n",
    "        print(f\"Labels shape: {data['labels'].shape}\")\n",
    "        print(f\"Unique labels: {np.unique(data['labels'])}\")\n",
    "        print(f\"Unique participants: {np.unique(data['participants'])}\")\n",
    "        \n",
    "        # Show a sample segment\n",
    "        print(f\"\\nSample segment (index 0):\")\n",
    "        print(f\"  Shape: {data['segments'][0].shape}\")\n",
    "        print(f\"  Label: {data['labels'][0]}\")\n",
    "        print(f\"  Participant: {data['participants'][0]}\")\n",
    "        print(f\"  Grid: {data['grids'][0]}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6f64dc-a719-44f1-ac19-659287ca3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "demonstrate_data_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f063f4f1-9335-4b71-9715-7852cc88d08e",
   "metadata": {},
   "source": [
    "## 7. Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d788b805-fdf2-4d17-856c-bd56be3c65b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_quality_checks():\n",
    "    \"\"\"\n",
    "    Perform quality checks on the standardized dataset\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"QUALITY CHECKS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    text\n",
    "    checks_passed = 0\n",
    "    total_checks = 0\n",
    "    \n",
    "    # Check 1: Data files exist\n",
    "    total_checks += 1\n",
    "    required_files = [\n",
    "        'backtapbench_data.npz',\n",
    "        'metadata.json',\n",
    "        'dataset_statistics.json',\n",
    "        'README.md'\n",
    "    ]\n",
    "    \n",
    "    missing_files = []\n",
    "    for file in required_files:\n",
    "        if not os.path.exists(os.path.join(OUTPUT_PATH, file)):\n",
    "            missing_files.append(file)\n",
    "    \n",
    "    if len(missing_files) == 0:\n",
    "        print(\"‚úÖ Check 1: All required files exist\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"‚ùå Check 1: Missing files: {missing_files}\")\n",
    "    \n",
    "    # Check 2: Segment shape consistency\n",
    "    total_checks += 1\n",
    "    try:\n",
    "        data = np.load(os.path.join(OUTPUT_PATH, 'backtapbench_data.npz'))\n",
    "        segments = data['segments']\n",
    "        \n",
    "        if len(set([s.shape for s in segments])) == 1:\n",
    "            print(f\"‚úÖ Check 2: All segments have consistent shape: {segments[0].shape}\")\n",
    "            checks_passed += 1\n",
    "        else:\n",
    "            print(f\"‚ùå Check 2: Inconsistent segment shapes\")\n",
    "            shapes = set([s.shape for s in segments])\n",
    "            for shape in shapes:\n",
    "                count = sum([1 for s in segments if s.shape == shape])\n",
    "                print(f\"   Shape {shape}: {count} segments\")\n",
    "    except:\n",
    "        print(\"‚ùå Check 2: Could not check segment shapes\")\n",
    "    \n",
    "    # Check 3: Label range\n",
    "    total_checks += 1\n",
    "    labels = data['labels']\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    if set(unique_labels) == set(range(9)):\n",
    "        print(f\"‚úÖ Check 3: All 9 grid positions represented (0-8)\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        missing = set(range(9)) - set(unique_labels)\n",
    "        extra = set(unique_labels) - set(range(9))\n",
    "        print(f\"‚ùå Check 3: Label issues\")\n",
    "        if missing:\n",
    "            print(f\"   Missing labels: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"   Extra labels: {extra}\")\n",
    "    \n",
    "    # Check 4: No NaN values\n",
    "    total_checks += 1\n",
    "    if not np.isnan(segments).any():\n",
    "        print(\"‚úÖ Check 4: No NaN values in segments\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        nan_count = np.isnan(segments).sum()\n",
    "        print(f\"‚ùå Check 4: Found {nan_count} NaN values in segments\")\n",
    "    \n",
    "    # Check 5: Balanced distribution\n",
    "    total_checks += 1\n",
    "    label_counts = np.bincount(labels.astype(int))\n",
    "    min_count = np.min(label_counts)\n",
    "    max_count = np.max(label_counts)\n",
    "    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')\n",
    "    \n",
    "    if imbalance_ratio < 1.5:  # Less than 50% imbalance\n",
    "        print(f\"‚úÖ Check 5: Labels reasonably balanced (ratio: {imbalance_ratio:.2f})\")\n",
    "        checks_passed += 1\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Check 5: Label imbalance detected (ratio: {imbalance_ratio:.2f})\")\n",
    "        for i, count in enumerate(label_counts):\n",
    "            print(f\"   Grid {i}: {count} samples\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(f\"QUALITY CHECK SUMMARY: {checks_passed}/{total_checks} passed\")\n",
    "    \n",
    "    if checks_passed == total_checks:\n",
    "        print(\"üéâ All quality checks passed! Dataset is ready for use.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Some checks failed. Review warnings above.\")\n",
    "    \n",
    "    return checks_passed, total_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426a320-8c0e-4bbb-bc35-78fbb828432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_passed, quality_total = perform_quality_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89bf4a-7d63-4968-ba20-ffad8e9ebc0e",
   "metadata": {},
   "source": [
    "## 8. Next Steps Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d7c3a8-587a-4567-bedb-8febd38a5ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_steps():\n",
    "    \"\"\"\n",
    "    Generate instructions for next steps\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"NEXT STEPS FOR BACKTAPBENCH\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    text\n",
    "    print(\"\\nüéØ YOUR STANDARDIZED DATASET IS READY!\")\n",
    "    print(f\"Location: {OUTPUT_PATH}\")\n",
    "    \n",
    "    print(\"\\nüìö NEXT NOTEBOOKS TO RUN:\")\n",
    "    print(\"\"\"\n",
    "    04_dtw_knn_benchmark.ipynb      - Your existing DTW+KNN algorithm\n",
    "    05_random_forest_features.ipynb - Random Forest on extracted features  \n",
    "    06_cnn_1d_implementation.ipynb  - 1D CNN on raw time-series\n",
    "    07_evaluation_framework.ipynb   - Unified evaluation of all methods\n",
    "    08_results_visualization.ipynb  - Generate figures and final results\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"üîß QUICK START EXAMPLE:\")\n",
    "    print(\"\"\"\n",
    "    # Load your standardized data\n",
    "    import numpy as np\n",
    "    data = np.load('backtapbench_standard/backtapbench_data.npz')\n",
    "    X = data['segments']  # Shape: (n_samples, 31, 10)\n",
    "    y = data['labels']    # Shape: (n_samples,)\n",
    "    \n",
    "    # Or load features for classical ML\n",
    "    import pandas as pd\n",
    "    features = pd.read_csv('backtapbench_standard/features/statistical_features.csv')\n",
    "    X_features = features.drop(['label', 'participant', 'grid_position', 'segment_id'], axis=1)\n",
    "    y_features = features['label']\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"\\nüìä DATASET STATISTICS:\")\n",
    "    try:\n",
    "        with open(os.path.join(OUTPUT_PATH, 'dataset_statistics.json'), 'r') as f:\n",
    "            stats = json.load(f)\n",
    "        \n",
    "        print(f\"  Total segments: {stats['total_segments']}\")\n",
    "        print(f\"  Unique participants: {stats['unique_participants']}\")\n",
    "        print(f\"  Segment shape: {stats['segment_shape_final']}\")\n",
    "        \n",
    "        # Show segments per participant\n",
    "        print(f\"\\n  Segments per participant:\")\n",
    "        for participant, count in stats['segments_per_participant'].items():\n",
    "            print(f\"    {participant}: {count}\")\n",
    "            \n",
    "    except:\n",
    "        print(\"  (Statistics not available)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ STANDARDIZATION COMPLETE!\")\n",
    "    print(\"=\" * 80_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b21b9-0cd6-404f-9753-ebc22e79e779",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_next_steps()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81cd88d-473a-4dee-bef3-0dae74cb154c",
   "metadata": {},
   "source": [
    "## 9. Export Configuration for Other Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c501b31-8a4e-40b2-b902-290b43a24443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export configuration for use in other notebooks\n",
    "config = {\n",
    "'data_path': DATA_PATH,\n",
    "'output_path': OUTPUT_PATH,\n",
    "'participants': PARTICIPANTS,\n",
    "'sensor_columns': SENSOR_COLS,\n",
    "'segment_length': SEGMENT_LENGTH,\n",
    "'sampling_rate': SAMPLING_RATE,\n",
    "'pre_samples': PRE_SAMPLES,\n",
    "'post_samples': POST_SAMPLES,\n",
    "'window_size': WINDOW_SIZE,\n",
    "'z_energy_threshold': Z_ENERGY_THRESHOLD,\n",
    "'gyro_threshold': GYRO_THRESHOLD,\n",
    "'debounce_samples': DEBOUNCE_SAMPLES\n",
    "}\n",
    "\n",
    "with open('dataset_config.json', 'w') as f:\n",
    "json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Configuration exported to: dataset_config.json\")\n",
    "\n",
    "# Save the standardizer object for reuse\n",
    "with open('standardizer.pkl', 'wb') as f:\n",
    "pickle.dump(standardizer, f)\n",
    "\n",
    "print(\"‚úÖ Standardizer object saved to: standardizer.pkl\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üéâ NOTEBOOK 3 COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nYour BackTapBench dataset is now standardized and ready for benchmarking!\")\n",
    "print(f\"Check the folder: {OUTPUT_PATH}\")\n",
    "print(\"\\nProceed to Notebook 4 to run your DTW+KNN algorithm on this standardized data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a2f4fc-fef1-4541-ba2b-4f27ff472f24",
   "metadata": {},
   "source": [
    "## **What This Notebook Does:**\n",
    "\n",
    "1. **Processes all 5 participants** and extracts tap segments\n",
    "2. **Creates standardized format** in `backtapbench_standard/` folder\n",
    "3. **Extracts features** for classical ML algorithms\n",
    "4. **Creates pre-defined splits** for evaluation\n",
    "5. **Generates comprehensive metadata** and documentation\n",
    "6. **Performs quality checks** on the standardized data\n",
    "7. **Provides loading examples** for future notebooks\n",
    "\n",
    "## **Output Structure Created:**\n",
    "backtapbench_standard/\n",
    "\n",
    "‚îú‚îÄ‚îÄ backtapbench_data.npz # Main dataset\n",
    "\n",
    "‚îú‚îÄ‚îÄ backtapbench_dataset.pkl # Python pickle\n",
    "\n",
    "‚îú‚îÄ‚îÄ metadata.json # Detailed metadata\n",
    "\n",
    "‚îú‚îÄ‚îÄ dataset_statistics.json # Statistics\n",
    "\n",
    "‚îú‚îÄ‚îÄ dataset_summary.csv # CSV summary\n",
    "\n",
    "‚îú‚îÄ‚îÄ README.md # Documentation\n",
    "\n",
    "‚îú‚îÄ‚îÄ dataset_distribution.png # Visualization\n",
    "\n",
    "‚îú‚îÄ‚îÄ features/ # Extracted features\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ statistical_features.csv\n",
    "\n",
    "‚îÇ ‚îú‚îÄ‚îÄ feature_matrix.npy\n",
    "\n",
    "‚îÇ ‚îî‚îÄ‚îÄ feature_names.json\n",
    "\n",
    "‚îî‚îÄ‚îÄ splits/ # Dataset splits\n",
    "\n",
    "‚îî‚îÄ‚îÄ dataset_splits.json\n",
    "\n",
    "## **Next Steps:**\n",
    "\n",
    "1. **Run this notebook** to create your standardized dataset\n",
    "2. **Check the output folder** to verify everything was created\n",
    "3. **Proceed to Notebook 4** (DTW+KNN benchmark) which will use this standardized data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c4b5b8-7421-49d2-aa85-1fccb4d803dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
